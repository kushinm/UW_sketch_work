{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt #For image plotting\n",
    "import numpy as np #basic data types and methods\n",
    "from skimage import draw   #To create shape arrays\n",
    "import sys                 #Just in case\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms,utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkquad(h, v):\n",
    "    #A quadralateral\n",
    "    o = np.zeros(shape = (4,3), dtype=np.int)\n",
    "    o[:,0] = [0,v,0,-v]\n",
    "    o[:,1] = [h, 0, -h, 0]\n",
    "    o[:,2] = 1\n",
    "    return o\n",
    "    \n",
    "def mkarc(h, v, r):\n",
    "    #An arc; r indicates rotation\n",
    "    o = np.zeros(shape=(3,3), dtype=np.int)\n",
    "    if(r==1):\n",
    "        o[:,0] = [0,v,0]\n",
    "        o[:,1] = [h,0,-h]\n",
    "    elif r==2:\n",
    "        o[:,0] = [h,0,-h]\n",
    "        o[:,1] = [0,-v,0]\n",
    "    elif r==3:\n",
    "        o[:,0] = [0,v,0]\n",
    "        o[:,1] = [-h,0,h]\n",
    "    elif r==4:\n",
    "        o[:,0] = [-v,0,v]\n",
    "        o[:,1] = [0,h,0]\n",
    "    \n",
    "    o[:,2] = 1\n",
    "        \n",
    "    return o\n",
    "\n",
    "def mkang(h,v,r):\n",
    "    #A right angle; r indicates rotation\n",
    "    o = np.zeros(shape=(2,3), dtype=np.int)\n",
    "    if(r==1):\n",
    "        o[:,0] = [v,0]\n",
    "        o[:,1] = [0,h]\n",
    "    elif r==2:\n",
    "        o[:,0] = [0,v]\n",
    "        o[:,1] = [h,0]\n",
    "    elif r==3:\n",
    "        o[:,0] = [-v,0]\n",
    "        o[:,1] = [0,h]\n",
    "    elif r==4:\n",
    "        o[:,0] = [0,-v]\n",
    "        o[:,1] = [h,0]\n",
    "        \n",
    "    o[:,2] = 1\n",
    "\n",
    "    return o  \n",
    "\n",
    "def mklin(h,v):\n",
    "    #A line\n",
    "    o = np.zeros(shape=(1,3), dtype=np.int)\n",
    "    o[0,:] = [v,h,1]\n",
    "    return o\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_from_coord(xy, picsize = 128, v=60, h=60, dt=0):\n",
    "    #xy: array of egocentric coordinates and pen state\n",
    "    #picsize: dimensions of image plane, assumed to be square\n",
    "    #v=vertical starting location for pen\n",
    "    #h = horizontal starting location for pen\n",
    "    #dt: drawing threshold--when pen state is above this value a line will be produced\n",
    "    \n",
    "    #Threshold pen state field\n",
    "    ps = xy[:,2] #copy real-valued pen state\n",
    "    ps[ps <= dt] = -1 \n",
    "    ps[ps > dt] = 1\n",
    "    xy[:,2] = ps\n",
    "    xy = xy.astype('int') #Make sure it is an integer array  \n",
    "    \n",
    "    ns = xy.shape[0] #Number of strokes\n",
    "    o = np.zeros(shape=(picsize, picsize,3)) #Image array\n",
    "    for i in np.arange(0,ns):\n",
    "        if(xy[i,2] > dt): #only draw if pen is down\n",
    "            rr, cc, val = draw.line_aa(v, h, v + xy[i,0], h + xy[i,1])\n",
    "            o[rr,cc,0] = val*255\n",
    "        v = v + xy[i,0]\n",
    "        h = h + xy[i,1]\n",
    "    \n",
    "    o[o>254] = 254 #Clip to max value\n",
    "    o = o/254\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute(relxy):\n",
    "    o = np.zeros(shape = relxy.shape)\n",
    "    o[0,:] = relxy[0,:]\n",
    "    for i in np.arange(o.shape[0]-1) + 1:\n",
    "        o[i,:] = o[i-1,:] + relxy[i,:]\n",
    "        \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rplace(relxy, wd=128, hg=128):\n",
    "    absxy = np.append([[0,0,0]], get_absolute(relxy), axis=0)\n",
    "    vmin = np.min(absxy[:,0]) #minimum vertical cell\n",
    "    vmax = np.max(absxy[:,0]) #maximum vertical cell\n",
    "    hmin = np.min(absxy[:,1]) #minimum horizontal cell\n",
    "    hmax = np.max(absxy[:,1]) #maximum horizontal cell\n",
    "    \n",
    "    vr = np.arange(-1 * vmin, hg - vmax, dtype=np.int) \n",
    "    hr = np.arange(-1 * hmin, wd - hmax, dtype=np.int) \n",
    "    \n",
    "    v = np.random.choice(vr,1)[0]\n",
    "    h = np.random.choice(hr,1)[0]\n",
    "    \n",
    "    return v, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipim(relxy):\n",
    "    o = relxy\n",
    "    o[:,1] = -1 * o[:, 1]\n",
    "    \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mktable(h= -1, v= -1, li= -1):\n",
    "    \n",
    "    if(h == -1): #Sample width if not specified from 20-50\n",
    "        tmp = np.arange(30, dtype=np.int) + 20\n",
    "        h = np.random.choice(tmp, 1)\n",
    "    \n",
    "    if(v == -1): #Sample height if not specified from 1 - width\n",
    "        tmp = np.arange(h -1 , dtype=np.int) + 1\n",
    "        v = np.random.choice(tmp, 1)\n",
    "        \n",
    "        \n",
    "    if(li== -1): #Sample if not specified\n",
    "        tmp = np.arange(h/3, dtype=np.int)\n",
    "        li = np.random.choice(tmp)\n",
    "        \n",
    "    o = np.zeros(shape=(5,3), dtype=np.int)\n",
    "    o[0,:] = mklin(h=h, v=0) #Surface\n",
    "    o[1,:] = [0, -h + li, -1] #Move left\n",
    "    o[2,:] = mklin(h=0, v=v) #First leg\n",
    "    o[3,:] = [-v, h - (2*li), -1] #Move to second leg\n",
    "    o[4,:] = mklin(h=0, v=v) #Second leg\n",
    "    \n",
    "    return o\n",
    "\n",
    "\n",
    "def mkstool(h= -1, v= -1, li= -1):\n",
    "    \n",
    "    if(v == -1): #Sample height if not specified from 20-50\n",
    "        tmp = np.arange(30, dtype=np.int) + 20\n",
    "        v = np.random.choice(tmp, 1)\n",
    "    \n",
    "    if(h == -1): #Sample width if not specified from 1 - height\n",
    "        tmp = np.arange(v -1 , dtype=np.int) + 1\n",
    "        h = np.random.choice(tmp, 1)\n",
    "        \n",
    "        \n",
    "    if(li== -1): #Sample if not specified\n",
    "        tmp = np.arange(h/3, dtype=np.int)\n",
    "        li = np.random.choice(tmp)\n",
    "        \n",
    "    o = np.zeros(shape=(5,3), dtype=np.int)\n",
    "    o[0,:] = mklin(h=h, v=0) #Surface\n",
    "    o[1,:] = [0, -h + li, -1] #Move left\n",
    "    o[2,:] = mklin(h=0, v=v) #First leg\n",
    "    o[3,:] = [-v, h - (2*li), -1] #Move to second leg\n",
    "    o[4,:] = mklin(h=0, v=v) #Second leg\n",
    "    \n",
    "    return o\n",
    "\n",
    "def mkchair(h= -1, v= -1, sh= -1):\n",
    "    \n",
    "    if(v == -1): #Sample height if not specified from 20-50\n",
    "        tmp = np.arange(30, dtype=np.int) + 20\n",
    "        v = np.random.choice(tmp, 1)\n",
    "    \n",
    "    if(h == -1): #Sample width if not specified from 1 - height\n",
    "        tmp = np.arange(v - 10 , dtype=np.int) + 5\n",
    "        h = np.random.choice(tmp, 1)\n",
    "              \n",
    "    if(sh== -1): #Sample seat height if not specified\n",
    "        tmp = np.arange(v/2, dtype=np.int) + np.round(v/10)\n",
    "        sh = np.random.choice(tmp)\n",
    "        \n",
    "    o = np.zeros(shape=(4,3), dtype=np.int)\n",
    "    o[0,:] = mklin(v=v, h=0) #Back\n",
    "    o[1,:] = [-sh, 0, -1] #Move up to seat height\n",
    "    o[2:4,:] = mkang(v=sh, h = h, r=2) #Seat and econd leg\n",
    "    \n",
    "    return o\n",
    "\n",
    "\n",
    "def mkmug(h= -1, v= -1, hsz= -1):\n",
    "    \n",
    "    if(v == -1): #Sample height if not specified from 20-50\n",
    "        tmp = np.arange(30, dtype=np.int) + 20\n",
    "        v = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if(h == -1): #Sample width if not specified from 1 - height\n",
    "        tmp = np.arange(v - 10 , dtype=np.int) + 5\n",
    "        h = np.random.choice(tmp, 1)[0]\n",
    "              \n",
    "    if(hsz== -1): #Sample handle size if not specified\n",
    "        tmp = np.arange(v/3, dtype=np.int) + np.int(v/3)\n",
    "        hsz = np.random.choice(tmp,1)[0]\n",
    "        \n",
    "    handloc = np.int((v - hsz)/2)\n",
    "    o = mkquad(v=v, h=h)\n",
    "    o = np.append(o, [[handloc, h, -1]], axis=0) #Move to handle top\n",
    "    o = np.append(o, mkarc(h = np.int(h/2), v = hsz, r=1), axis=0)\n",
    "    \n",
    "    return o\n",
    "\n",
    "def mkcase(h= -1, v= -1, hsz= -1):\n",
    "    \n",
    "    if(h == -1): #Sample height if not specified from 20-50\n",
    "        tmp = np.arange(30, dtype=np.int) + 20\n",
    "        h = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if(v == -1): #Sample width if not specified from 1 - height\n",
    "        tmp = np.arange(h - 10 , dtype=np.int) + 5\n",
    "        v = np.random.choice(tmp, 1)[0]\n",
    "              \n",
    "    if(hsz== -1): #Sample handle size if not specified\n",
    "        tmp = np.arange(v/3, dtype=np.int) + np.int(v/3)\n",
    "        hsz = np.random.choice(tmp,1)[0]\n",
    "        \n",
    "    handloc = np.int((h - hsz)/2)\n",
    "    o = mkquad(v=v, h=h)\n",
    "    o = np.append(o, [[0, handloc, -1]], axis=0) #Move to handle top\n",
    "    o = np.append(o, mkarc(h = hsz, v = np.int(v/3), r=4), axis=0)\n",
    "    \n",
    "    return o\n",
    "\n",
    "def mkbird(hd = -1, bd = -1, nc = -1, bk = -1, lg = -1):\n",
    "    if hd == -1: #Sample head size\n",
    "        tmp = np.arange(10, dtype=np.int) + 5\n",
    "        hd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if bd == -1: #Sample body size\n",
    "        tmp = np.arange(hd, dtype=np.int) + hd + 5\n",
    "        bd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if nc == -1: #Sample neck length\n",
    "        tmp = np.arange(2 * hd, dtype=np.int)\n",
    "        nc = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if bk == -1: #Sample beak length\n",
    "        tmp = np.arange(2 * hd, dtype=np.int) + 3\n",
    "        bk = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if lg == -1: #Sample leg length\n",
    "        tmp = np.arange(2 * hd, dtype=np.int) + 3\n",
    "        lg = np.random.choice(tmp, 1)[0]\n",
    "\n",
    "    bp = np.int(hd * .8) #Beak position\n",
    "    o = mkquad(v=hd, h=hd) #draw head\n",
    "    o = np.append(o, [[bp, 0, -1]], axis=0) #Move to beak\n",
    "    o = np.append(o, [[0, -1 * bk, 1]], axis=0) #Draw beak\n",
    "    \n",
    "    o = np.append(o, [[hd - bp, bk + hd, -1]], axis=0) #Move to neck\n",
    "    o = np.append(o, mklin(v=nc, h=0), axis=0) #Draw neck\n",
    "    o = np.append(o, mkquad(bd, bd), axis = 0) #Draw body\n",
    "\n",
    "    lp = np.int(bd/2) - 4\n",
    "    o = np.append(o, [[bd, lp, -1]], axis=0) #Move to leg 1\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 1\n",
    "    o = np.append(o, [[-lg, 8, -1]], axis=0) #Move to leg 2\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 2\n",
    "    \n",
    "    return o\n",
    "        \n",
    "        \n",
    "def mksheep(hd = -1, bd = -1, nc = -1, lg = -1):\n",
    "    if hd == -1: #Sample head size\n",
    "        tmp = np.arange(10, dtype=np.int) + 5\n",
    "        hd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if bd == -1: #Sample body size\n",
    "        tmp = np.arange(hd, dtype=np.int) + hd + 5\n",
    "        bd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if nc == -1: #Sample neck length\n",
    "        tmp = np.arange(hd/2, dtype=np.int) + np.int(hd/2)\n",
    "        nc = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if lg == -1: #Sample leg length\n",
    "        tmp = np.arange(hd, dtype=np.int) + np.int(hd/2)\n",
    "        lg = np.random.choice(tmp, 1)[0]\n",
    "\n",
    "    o = mkquad(v=hd, h=np.int(hd * 1.3)) #draw head\n",
    "    o = np.append(o, [[hd, np.int(hd * 1.3), -1]], axis=0) #Move to neck\n",
    "    o = np.append(o, mklin(v=nc, h=0), axis=0) #Draw neck\n",
    "    o = np.append(o, mkquad(v=bd, h=np.int(bd * 1.3)), axis = 0) #Draw body\n",
    "\n",
    "    o = np.append(o, [[bd, 0, -1]], axis=0) #Move to leg 1\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 1\n",
    "    o = np.append(o, [[-lg, np.int(bd * 1.3), -1]], axis=0) #Move to leg 2\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 2\n",
    "    \n",
    "    return o\n",
    "        \n",
    "\n",
    "def mkdog(hd = -1, bd = -1, nc = -1, bk = -1, lg = -1):\n",
    "    if hd == -1: #Sample head size\n",
    "        tmp = np.arange(10, dtype=np.int) + 5\n",
    "        hd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if bd == -1: #Sample body size\n",
    "        tmp = np.arange(hd, dtype=np.int) + 2 * hd\n",
    "        bd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if nc == -1: #Sample neck length\n",
    "        tmp = np.arange(hd/2, dtype=np.int) + np.int(hd/2)\n",
    "        nc = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if lg == -1: #Sample leg length\n",
    "        tmp = np.arange(hd, dtype=np.int) + np.int(hd/2)\n",
    "        lg = np.random.choice(tmp, 1)[0]\n",
    "\n",
    "    o = mkquad(v=hd, h=np.int(hd * 1.3)) #draw head\n",
    "    o = np.append(o, [[hd, np.int(hd * 1.3), -1]], axis=0) #Move to neck\n",
    "    o = np.append(o, mklin(v=nc, h=0), axis=0) #Draw neck\n",
    "    o = np.append(o, mklin(v=0, h=bd), axis = 0) #Draw body\n",
    "\n",
    "    o = np.append(o, [[0, -bd, -1]], axis=0) #Move to leg 1\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 1\n",
    "    o = np.append(o, [[-lg, bd, -1]], axis=0) #Move to leg 2\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 2\n",
    "    \n",
    "    return o\n",
    "        \n",
    "def mkliz(h= -1, v= -1, bd= -1):\n",
    "    \n",
    "    if(h == -1): #Sample width if not specified from 20-50\n",
    "        tmp = np.arange(10, dtype=np.int) + 5\n",
    "        h = np.random.choice(tmp, 1)[0]\n",
    "    \n",
    "    if(v == -1): #Sample height if not specified from 1 - width\n",
    "        tmp = np.arange(h, dtype=np.int) + 3\n",
    "        v = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "        \n",
    "    if(bd== -1): #Sample if not specified\n",
    "        tmp = np.arange(h*3, dtype=np.int) + h*2\n",
    "        bd = np.random.choice(tmp,1)[0]\n",
    "        \n",
    "    o = mkquad(h,h) #Draw head\n",
    "    o = np.append(o, [[np.int(h/2), h, -1]], axis = 0) #Move to body\n",
    "    o = np.append(o, mklin(h=bd, v=0), axis = 0) #Draw body\n",
    "    o = np.append(o, [[0, np.int(-1 * bd * 7/15), -1]], axis=0)  #Move to back leg\n",
    "    o = np.append(o, mklin(h=0, v=v), axis = 0) #draw leg\n",
    "    o = np.append(o, [[-v, np.int(-1 * bd * 5/15), -1]], axis=0)  #Move to front leg\n",
    "    o = np.append(o, mklin(h=0, v=v), axis = 0) #draw leg\n",
    "    \n",
    "    return o\n",
    "        \n",
    "\n",
    "def mkpig(hd = -1, bd = -1, lg = -1):\n",
    "    \n",
    "    if hd == -1: #Sample head size\n",
    "        tmp = np.arange(10, dtype=np.int) + 10\n",
    "        hd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "    if bd == -1: #Sample body size\n",
    "        tmp = np.arange(hd, dtype=np.int) + hd + 5\n",
    "        bd = np.random.choice(tmp, 1)[0]\n",
    "        \n",
    "        \n",
    "    if lg == -1: #Sample leg length\n",
    "        tmp = np.arange(hd, dtype=np.int) + np.int(hd/2)\n",
    "        lg = np.random.choice(tmp, 1)[0]\n",
    "\n",
    "    sn = np.int(hd/3) #Snout size\n",
    "    o = mkquad(v=hd, h=hd) #draw head\n",
    "    o = np.append(o, [[2 * sn, -sn, -1]], axis=0) #Move to snout   \n",
    "    o = np.append(o, mkquad(v=sn, h=sn), axis=0) #Draw snout\n",
    "    o = np.append(o, [[-bd + sn, hd + sn, -1]], axis=0) #Move to body   \n",
    "    \n",
    "    o = np.append(o, mkquad(v=bd, h=np.int(bd * 1.3)), axis = 0) #Draw body\n",
    "\n",
    "    o = np.append(o, [[bd, 0, -1]], axis=0) #Move to leg 1\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 1\n",
    "    o = np.append(o, [[-lg, np.int(bd * 1.3), -1]], axis=0) #Move to leg 2\n",
    "    o = np.append(o, mklin(v=lg, h=0), axis=0) #Draw leg 2\n",
    " \n",
    "    return o\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_img(choose = None):\n",
    "    if not choose:\n",
    "        i = np.random.randint(0,10)\n",
    "    else:\n",
    "        i = choose\n",
    "    if(i==0):\n",
    "        o = mktable()\n",
    "    elif(i==1):\n",
    "        o = mkchair()\n",
    "    elif(i==2):\n",
    "        o = mkstool()\n",
    "    elif(i==3):\n",
    "        o = mkmug()\n",
    "    elif(i==4):\n",
    "        o = mkcase()\n",
    "    elif(i==5):\n",
    "        o = mkdog()\n",
    "    elif(i==6):\n",
    "        o = mksheep()\n",
    "    elif(i==7):\n",
    "        o = mkbird()\n",
    "    elif(i==8):\n",
    "        o = mkliz()\n",
    "    elif(i==9):\n",
    "        o = mkpig()\n",
    "\n",
    "    return o\n",
    "\n",
    "def get_batch(bsize=200, ns = 20, picsize=128, rloc=False, flip=False, not_rand=False, choose=0):\n",
    "    xtrn = np.zeros(shape = (bsize,picsize,picsize,3)) #array for training image\n",
    "    trnxy = np.zeros(shape=(bsize, ns, 3), dtype=np.int)  #array for sequence of training coordinates\n",
    "    trnxy[:,:,2] = -1 #Default is pen up\n",
    "    imloc = np.zeros(shape = (bsize, 2), dtype = np.int)\n",
    "\n",
    "    for i in np.arange(0,bsize):\n",
    "        if (not_rand==True):\n",
    "             relxy = get_rand_img(choose=choose)\n",
    "        else:    \n",
    "            relxy = get_rand_img()\n",
    "\n",
    "        if(np.random.randint(0,2)==1 and flip):\n",
    "            relxy = flipim(relxy)\n",
    "\n",
    "        nstep = relxy.shape[0]\n",
    "        if rloc:\n",
    "            v, h = rplace(relxy, wd=picsize, hg=picsize)\n",
    "        else:\n",
    "            v = 30\n",
    "            h = 30\n",
    "            \n",
    "        xtrn[i,:,:,:] = get_shape_from_coord(relxy, v=v, h=h)\n",
    "        trnxy[i,0:nstep,:] = relxy\n",
    "        imloc[i,:] = [v,h]\n",
    "\n",
    "    \n",
    "    return xtrn, trnxy, imloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensorCustom(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        _x, _y = sample\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        print(_x.shape)\n",
    "        _x = _x.transpose((2, 0, 1))\n",
    "        _y = _y.transpose((2, 0, 1))\n",
    "        return ((_x,_y).to(device))\n",
    "    \n",
    "class sketchdata(Dataset):\n",
    "    def __init__(self, _x,_y,transform=None):\n",
    "        self.images=[]\n",
    "        self.coordinates=[]\n",
    "      \n",
    "        self.transform = transform\n",
    "        \n",
    "        for sketch in _x:\n",
    "            self.images.append(torch.from_numpy(np.asarray(sketch).transpose((2, 0, 1))).float().to(device))\n",
    "        for coord in _y:\n",
    "            self.coordinates.append(torch.from_numpy(coord).float().to(device))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #return \n",
    "        images = self.images[idx]\n",
    "        coordinates = self.coordinates[idx]\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            images = self.transform(images)\n",
    "            \n",
    "            \n",
    "\n",
    "        return images, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "bsize = 1000\n",
    "ns=20\n",
    "picsz=128\n",
    "\n",
    "xtrn = np.zeros(shape=(bsize, ns, picsz, picsz, 3))\n",
    "xtst = np.zeros(shape = (bsize, ns, picsz, picsz, 3))\n",
    "\n",
    "#Get a batch of training data\n",
    "x1, ytrn, imloc_trn = get_batch(bsize)\n",
    "for i in np.arange(0,ns):\n",
    "    xtrn[:,i,:,:] = x1 #Copy input image to each timeslice\n",
    "\n",
    "\n",
    "#Get a batch of testing data\n",
    "x2, ytst, imloc_tst = get_batch(bsize)\n",
    "for i in np.arange(0,ns):\n",
    "    xtst[:,i,:,:] = x2 #Copy input image to each timeslice\n",
    "\n",
    "\n",
    "\n",
    "#a,b = gen_sketch_data(num_sketches=20)\n",
    "#ytrn= torch.tensor(ytrn, dtype=torch.uint8)\n",
    "traindata =  sketchdata(x1,ytrn )\n",
    "\n",
    "\n",
    "\n",
    "#c,d = gen_sketch_data(num_sketches=10)\n",
    "#ytst= torch.tensor(ytst, dtype=torch.uint8)\n",
    "testdata =  sketchdata(x2,ytst )\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=50,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=50,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell architecture\n",
    "\n",
    "<div>\n",
    "<img src=\"LSTMCell.png\" width=\"700\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention with LSTM\n",
    "<div>\n",
    "<img src=\"Attention.png\" width=\"700\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class EncoderSimple(nn.Module):\n",
    "    def __init__(self, embed_size = 2):\n",
    "        super(EncoderSimple, self).__init__()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(3,8,3)\n",
    "        self.conv2 = nn.Conv2d(8,16,3)\n",
    "        self.conv3 = nn.Conv2d(16,32,3)\n",
    "\n",
    "        \n",
    "        # add another fully connected layer\n",
    "        self.fc_mid = nn.Linear(in_features=6272, out_features=512)\n",
    "        self.fc_out = nn.Linear(in_features=512, out_features=256)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # activation layers\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        x = self.relu(self.conv1(images))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0),-1) ## flatten\n",
    "        \n",
    "        # pass through the fully connected layer\n",
    "        x = self.relu(self.fc_mid(x))\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size=2, hidden_size=256, coord_size=3, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.coord_size = coord_size\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=3, hidden_size=self.hidden_size)\n",
    "    \n",
    "       # linear layers\n",
    "        self.fc_hs = nn.Linear(in_features=256, out_features=self.hidden_size) ### layer to convert flattened image latent vector to a smaller vector to initialize hidden state\n",
    "        \n",
    "\n",
    "        self.fc_rnn_inp1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc_rnn_inp2 = nn.Linear(in_features=128, out_features=3) ### layer that input to RNN at each timestep to a 128 dimensional vector\n",
    "\n",
    "        self.fc_mid = nn.Linear(in_features=self.hidden_size, out_features=128, bias=True) ### hidden that takes the output of the LSTM cell and converts it to a smaller size\n",
    "        self.fc_out = nn.Linear(in_features=128, out_features=self.coord_size) ### hidden layer that takes the outputs from the above hidden layer and converts it to an output for the sketcher\n",
    "    \n",
    "        # embedding layer\n",
    "        #self.embed = nn.Embedding(num_embeddings=self.coord_size, embedding_dim=self.embed_size)\n",
    "    \n",
    "        # activations\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self,features,coordinates=[],coord_length=20, inference=False,teacher_forcing_ratio = 0):\n",
    "        \n",
    "        # batch size\n",
    "        batch_size = features.size(0) ### number of images\n",
    "        \n",
    "        # init the hidden and cell states with image features\n",
    "        \n",
    "        # hidden_state = self.tanh(self.fc_hs(features)) \n",
    "        # cell_state = self.tanh(self.fc_hs(features))\n",
    "\n",
    "        hidden_state = torch.zeros((batch_size,self.hidden_size),device=device)\n",
    "        cell_state = torch.zeros((batch_size,self.hidden_size),device=device)\n",
    "\n",
    "        \n",
    "        #z = torch.cat([coordinates, features.unsqueeze(0).expand(1,coordinates.size(1), features.size(1))], 2)\n",
    "    \n",
    "#         print(features.size(),torch.tensor([0,0]).expand(1,2).float().size() )\n",
    "        #_features = torch.cat([features, torch.tensor([0,0]).expand(batch_size,2).float()], 1)\n",
    "\n",
    "        # embed the captions\n",
    "        #captions_embed = self.embed(captions)\n",
    "        \n",
    "        \n",
    "        if inference==False: ### training forward pass\n",
    "            # define the output tensor placeholder currently at 20 strokes max\n",
    "            outputs = torch.empty((batch_size, coord_length, self.coord_size))\n",
    "            # pass the caption word by word\n",
    "            for t in range(coord_length):\n",
    "                \n",
    "                #teacher_force = random.random() < teacher_forcing_ratio ## do we teacher force?  https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/DecoderRNN.py\n",
    "                teacher_force = True\n",
    "                # for the first time step, the input is the feature vector\n",
    "                if t == 0:\n",
    "                    #inp = torch.tensor([0,0]).expand(batch_size,2).float()\n",
    "                    inp = self.fc_rnn_inp1(features)\n",
    "                    inp = self.fc_rnn_inp2(inp)\n",
    "                    hidden_state, cell_state = self.lstm_cell(inp, (hidden_state, cell_state))\n",
    "\n",
    "                # for the 2nd+ time step, using teacher forcer\n",
    "                else:\n",
    "                    #feat_coords = torch.cat([features,inp],axis=1)\n",
    "                    #inp = coordinates[:,t,:] if teacher_force else mid\n",
    "                    inp=out\n",
    "                    hidden_state, cell_state = self.lstm_cell(inp, (hidden_state, cell_state))\n",
    "\n",
    "                # output of the attention mechanism\n",
    "                mid = self.fc_mid(hidden_state)  ### output after first hidden layer\n",
    "                out = self.fc_out(mid) ### output after second hidden layer\n",
    "\n",
    "                # build the output tensor\n",
    "                outputs[:, t, :] = out\n",
    "        else:\n",
    "            outputs = torch.empty((batch_size, coord_length, self.coord_size))\n",
    "            for t in range(coord_length):\n",
    "                if t == 0:\n",
    "                    inp = self.fc_rnn_inp1(features)\n",
    "                    inp = self.fc_rnn_inp2(features)\n",
    "                    hidden_state, cell_state = self.lstm_cell(inp, (hidden_state, cell_state))\n",
    "                    mid = self.fc_mid(hidden_state)\n",
    "                    out = self.relu(self.fc_out(mid))\n",
    "\n",
    "                \n",
    "                else:\n",
    "                    #print(features.size(),out.size())\n",
    "                    #feat_coords = torch.cat([features,out],axis=1)\n",
    "                    hidden_state, cell_state = self.lstm_cell(out, (hidden_state, cell_state))\n",
    "                    mid = self.fc_mid(hidden_state)\n",
    "                    out = self.fc_out(mid)\n",
    "\n",
    "                # output of the attention mechanism\n",
    "\n",
    "\n",
    "                # build the output tensor\n",
    "                outputs[:, t, :] = out\n",
    "    \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class DecoderSimple(nn.Module):\n",
    "    def __init__(self, coord_length=20, hidden_size=256, coord_size=3, num_layers=1):\n",
    "        super(DecoderSimple, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.coord_size = coord_size\n",
    "        self.coord_length = coord_length\n",
    "        \n",
    "        # lstm \n",
    "        self.rnn = nn.LSTM(256, self.hidden_size, 1, batch_first=True)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_hs = nn.Linear(in_features=256, out_features=self.hidden_size)\n",
    "        self.fc_rnn_inp = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc_mid = nn.Linear(in_features=self.hidden_size, out_features=128, bias=True)\n",
    "        self.fc_out = nn.Linear(in_features=128, out_features=self.coord_size)\n",
    "    \n",
    "        # embedding layer\n",
    "        #self.embed = nn.Embedding(num_embeddings=self.coord_size, embedding_dim=self.embed_size)\n",
    "    \n",
    "        # activations\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self,features,coord_length=20,inference=False,coords=[]):\n",
    "    \n",
    "        \n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        feat_dim = features.size(1)\n",
    "        \n",
    "        input = features.unsqueeze(1).expand(batch_size, coord_length, feat_dim)\n",
    "        \n",
    "        # init the hidden and cell states to zeros(device)\n",
    "        hidden_state = torch.zeros((1,batch_size,self.hidden_size),device=device)\n",
    "        cell_state = torch.zeros((1,batch_size,self.hidden_size),device=device)\n",
    "        lstm_out, (hn, cn) = self.rnn(input, (hidden_state, cell_state))\n",
    "        mid = self.fc_mid(lstm_out)\n",
    "        outputs =  self.fc_out(mid)\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim=10, embed_dim=3, decoder_dim=256, encoder_dim=256, dropout=0.5,\n",
    "     coord_length=20, hidden_size=256, coord_size=3):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        #self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        #self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        #self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        #self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.decode_step = nn.LSTMCell(embed_dim, decoder_dim, bias=True) \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, embed_dim)  # linear layer to find scores over vocabulary\n",
    "\n",
    "        self.fc_mid = nn.Linear(in_features=self.decoder_dim, out_features=128, bias=True)\n",
    "        self.fc_out = nn.Linear(in_features=128, out_features=self.embed_dim)\n",
    "\n",
    "\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "        self.rnn_in = nn.Linear(encoder_dim,embed_dim)\n",
    "        \n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        #self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    # def load_pretrained_embeddings(self, embeddings):\n",
    "    #     \"\"\"\n",
    "    #     Loads embedding layer with pre-trained embeddings.\n",
    "    #     :param embeddings: pre-trained embeddings\n",
    "    #     \"\"\"\n",
    "    #     self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    # def fine_tune_embeddings(self, fine_tune=True):\n",
    "    #     \"\"\"\n",
    "    #     Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "    #     :param fine_tune: Allow?\n",
    "    #     \"\"\"\n",
    "    #     for p in self.embedding.parameters():\n",
    "    #         p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        #mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, coord_length=20,coord_size=3,coords=[],inference=False):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        #encoder_dim = encoder_out.size(-1)\n",
    "        encoder_dim = encoder_out.size(0)\n",
    "        # vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        #encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        #num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        # caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        # encoder_out = encoder_out[sort_ind]\n",
    "        # encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # # Embedding\n",
    "        # embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        embeddings = self.rnn_in(encoder_out)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        # decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, coord_length, coord_size).to(device)\n",
    "        #alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        if inference==False:\n",
    "            for t in range(coord_length):\n",
    "            #batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            #attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "            #                                                    h[:batch_size_t])\n",
    "            #gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            #attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "                if t==0:\n",
    "                    # h, c = self.decode_step(\n",
    "                    #     torch.cat([embeddings, encoder_out], dim=1),\n",
    "                    #     (h, c))  # (batch_size_t, decoder_dim)\n",
    "                    h, c = self.decode_step(embeddings,(h, c))\n",
    "\n",
    "\n",
    "                else: \n",
    "                    #print('encoder shape= ',encoder_out.size(),'coords',coords.size() )\n",
    "                    # h, c = self.decode_step(\n",
    "                    #     torch.cat([coords[:,t-1,:], encoder_out], dim=1),\n",
    "                    #     (h, c))  # (batch_size_t, decoder_dim)\n",
    "                    h, c = self.decode_step(coords[:,t-1,:],(h, c))\n",
    "\n",
    "                #preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "                mid = self.fc_mid(h)\n",
    "                preds =  self.fc_out(mid)\n",
    "                \n",
    "                predictions[:, t, :] = preds\n",
    "        else:        \n",
    "\n",
    "            for t in range(coord_length):\n",
    "                #batch_size_t = sum([l > t for l in decode_lengths])\n",
    "                #attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                #                                                    h[:batch_size_t])\n",
    "                #gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "                #attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "                if t==0:\n",
    "                    # h, c = self.decode_step(\n",
    "                    #     torch.cat([embeddings, encoder_out], dim=1),\n",
    "                    #     (h, c))  # (batch_size_t, decoder_dim)\n",
    "                    h, c = self.decode_step(embeddings,(h, c))\n",
    "                else: \n",
    "                    # h, c = self.decode_step(\n",
    "                    #     torch.cat([preds, encoder_out], dim=1),\n",
    "                    #     (h, c))  # (batch_size_t, decoder_dim)\n",
    "                    h, c = self.decode_step(preds,(h, c))\n",
    "\n",
    "                #preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "                mid = self.fc_mid(h)\n",
    "                preds =  self.fc_out(mid)\n",
    "                predictions[:, t, :] = preds\n",
    "\n",
    "            #alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")\n",
    "\n",
    "enc = EncoderSimple().to(device)\n",
    "dec = DecoderWithAttention().to(device)\n",
    "#dec = DecoderSimple().to(device)\n",
    "\n",
    "\n",
    "\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params,weight_decay=1e-5)\n",
    "criterion =  nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, cd = next(iter(trainloader))\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the losses for vizualization\n",
    "\n",
    "losses = list()\n",
    "val_losses = list()\n",
    "\n",
    "num_epochs=25\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        # zero the gradients\n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "        \n",
    "        # set decoder and encoder into train mode\n",
    "        enc.train()\n",
    "        dec.train()\n",
    "        \n",
    "#         # Randomly sample a caption length, and sample indices with that length.\n",
    "#         indices = train_data_loader.dataset.get_train_indices()\n",
    "        \n",
    "#         # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "#         new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "#         train_data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, coordinates = data\n",
    "       # images = images.float()\n",
    "       # coordinates = coordinates.float()\n",
    "        \n",
    "        # make the captions for targets and teacher forcer\n",
    "#         coordinates_target = coordinates.to(device)\n",
    "#         tmp = torch.cat((torch.zeros(coordinates.size(0),1,2), coordinates), axis=1)\n",
    "#         coordinates_train = tmp[:, :tmp.shape[1]-1].to(device)\n",
    "        \n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        coordinates = coordinates.to(device)\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = enc(images).to(device)\n",
    "#         outputs = dec(features, coordinates_train, inference=True)\n",
    "        outputs = dec(features,inference=True,coords=coordinates).to(device)\n",
    "\n",
    "    \n",
    "        # Calculate the batch loss\n",
    "        #loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
    "#         loss = criterion(outputs, coordinates_target)\n",
    "        loss = criterion(outputs, coordinates)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # - - - Validate - - -\n",
    "        # turn the evaluation mode on\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "    for i, data in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # set the evaluation mode\n",
    "            enc.eval()\n",
    "            dec.eval()\n",
    "\n",
    "            # get the validation images and captions\n",
    "            val_images, val_coordinates = data\n",
    "            # val_coordinates= val_coordinates.float()\n",
    "            # val_images= val_images.float()\n",
    "            \n",
    "            # define the captions\n",
    "#             coordinates_target = val_coordinates.to(device)\n",
    "#             tmp = torch.cat((torch.zeros(val_coordinates.size(0),1,2), val_coordinates), axis=1)\n",
    "#             coordinates_train = tmp[:, :tmp.shape[1]-1].to(device)\n",
    "        \n",
    "\n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            val_images = val_images.to(device)\n",
    "            val_coordinates = val_coordinates.to(device)\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = enc(val_images).to(device)\n",
    "            outputs = dec(features, inference=True).to(device)\n",
    "\n",
    "            # Calculate the batch loss.\n",
    "            #val_loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
    "            #val_loss = criterion(outputs, coordinates_target)\n",
    "            val_loss = criterion(outputs, val_coordinates)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        # save the losses\n",
    "        np.save('losses', np.array(losses))\n",
    "        np.save('val_losses', np.array(val_losses))\n",
    "        \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Val Loss: %.4f' % (epoch, num_epochs, i, len(trainloader), loss.item(), val_loss.item())\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        #sys.stdout.flush()\n",
    "            \n",
    "    # Save the weights.\n",
    "#     if epoch % save_every == 0:\n",
    "#         print(\"\\nSaving the model\")\n",
    "#         torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pth' % epoch))\n",
    "#         torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pth' % epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(val_losses))\n",
    "y = val_losses\n",
    "z = losses\n",
    "\n",
    "ax.plot(x,y)\n",
    "ax.plot(x,z)\n",
    "ax.legend(['validation','training'])\n",
    "#plt.plot(np.arange(len(val_losses)),val_losses, 'go--', linewidth=2, markersize=1)\n",
    "plt.title('Losses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(testloader))\n",
    "tru = batch[0][0:9].float()\n",
    "truc = batch[1][0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "gridsz = 3\n",
    "gsz = 3 #plotting grid size\n",
    "s = np.random.choice(np.arange(0,100), gridsz * gridsz) #random selection of items\n",
    "#tru = x2[s,:,:,:]  #get inputs from test set\n",
    "loc = imloc_tst[s,:]   #Get starting locations of test items\n",
    "pred_sm = np.array(dec(enc(tru),inference=True).detach())\n",
    "#pred_ae= np.array(aedec(aeenc(tru)).detach())\n",
    "\n",
    "#pred_hs=np.array(hdec(henc(tru))[1].detach())\n",
    "#pred_hae= np.array(hdec(henc(tru))[0].detach())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot model image and image regenerated from model output sequence\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,gsz):\n",
    "    for j in range(0,gsz):\n",
    "        plt.subplot2grid((gsz,gsz), (i,j))\n",
    "        indx = i * gsz + j\n",
    "        tmp = tru[indx,:,:,:].numpy().transpose((1,2,0))\n",
    "        fr = get_shape_from_coord(np.squeeze(pred_sm[indx,:,:]), v=loc[indx, 0], h=loc[indx, 1])\n",
    "        tmp[:,:,1] = fr[:,:,0]\n",
    "        \n",
    "        #Uncomment next line to see just the model output drawing\n",
    "        #tmp[:,:,0] = 0\n",
    "\n",
    "        fig = plt.imshow(tmp)\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=.1, hspace=.1)\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot model image and image regenerated from model output sequence\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,gsz):\n",
    "    for j in range(0,gsz):\n",
    "        plt.subplot2grid((gsz,gsz), (i,j))\n",
    "        indx = i * gsz + j\n",
    "        tmp = tru[indx,:,:,:].numpy().transpose((1,2,0))\n",
    "        plt.imshow(tmp)\n",
    "        fr = get_shape_from_coord(np.squeeze(pred_sm[indx,:,:]), v=loc[indx, 0], h=loc[indx, 1])\n",
    "        tmp[:,:,1] = fr[:,:,0]\n",
    "        \n",
    "        #Uncomment next line to see just the model output drawing\n",
    "        tmp[:,:,0] = 0\n",
    "\n",
    "        fig = plt.imshow(tmp)\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=.1, hspace=.1)\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sketch_models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948ddb06426d110fb8709d965d8d26aeed62a2fade135e77a31f81eb95224669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
