{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for creating image to image autoencoders for sketches using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Author: Kushin Mukherjee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using python 3 plus the latest versions of all the packages listed below. Be sure to update before running this nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import package\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from importlib import reload\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import svgpathtools\n",
    "import os.path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "import svg_rendering_helpers as srh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "code_dir = os.getcwd()\n",
    "plot_dir = os.path.join(proj_dir,'plots')\n",
    "data_dir = os.path.join(proj_dir,'data')\n",
    "\n",
    "\n",
    "if not os.path.exists(code_dir):\n",
    "    os.makedirs(code_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "    \n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# if svg_rendering_helpers.py not in sys.path:\n",
    "#     sys.path.append(os.path.join(proj_dir,svg_rendering_helpers.py)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv(os.path.join(data_dir,'semantic_parts_annotated_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.sketch_svg_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(string):\n",
    "    split_list = string.split(\"'\")\n",
    "    l=[\", u\",\"[u\",\"]\"]\n",
    "    out = [x for x in split_list if x not in l]\n",
    "    return(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.sketch_svg_string = D.sketch_svg_string.apply(listify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D.sketch_svg_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cats = np.unique(D.category)\n",
    "unique_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spline-level df where the modal label is set as the 'true' label for any given spline\n",
    "spline_df= D.groupby('spline_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "spline_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "##Creating a stroke-level dataframe that takes the mode value of annotation for its children splines to set as its\n",
    "##label value\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "stroke_svgs=OrderedDict()\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in np.unique(DS['sketch_id']):\n",
    "        DSS=DS[DS['sketch_id']==sketch]\n",
    "        for stroke in np.unique(DSS['stroke_num']):\n",
    "            DSA=DSS[DSS['stroke_num']==stroke]\n",
    "            DSA=DSA.reset_index()\n",
    "            stroke_svgs[DSA['stroke_id'][0]] = DSA['sketch_svg_string'][0][stroke]\n",
    "\n",
    "            \n",
    "            \n",
    "stroke_svg_df= pd.DataFrame.from_dict(stroke_svgs, orient='index')    \n",
    "stroke_group_data= D.groupby('stroke_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "labels= pd.DataFrame(stroke_group_data[['sketch_id','label','stroke_num','condition','target','category','outcome']])\n",
    "stroke_df=pd.merge(stroke_svg_df,labels,left_index=True, right_index =True)\n",
    "stroke_df.reset_index(level=0, inplace=True)\n",
    "stroke_df=stroke_df.rename(index=str, columns={\"index\": \"stroke_id\", 0: \"svg\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data for triplets task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sketches = [\n",
    "'3058-fb4fe740-d862-453b-a08f-44375a040165_21',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_8',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_12',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_23',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_24',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_15',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_24',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_16',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_20',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_22',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_7',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_13',\n",
    "'6311-cd21a68a-f1df-4290-b744-b0c7c7c60ed8_5',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_32'\n",
    "]\n",
    "\n",
    "stroke_df = stroke_df[~stroke_df['sketch_id'].isin(bad_sketches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1022)\n",
    "sample_sketches= []\n",
    "\n",
    "for this_cat in unique_cats:\n",
    "    cat_df = stroke_df[stroke_df['category']== this_cat]\n",
    "    unique_items = np.unique(cat_df['target'])\n",
    "    for this_item in unique_items:\n",
    "        item_df = cat_df[cat_df['target']==this_item]\n",
    "        unique_conds = np.unique(item_df['condition'])\n",
    "        for this_cond in unique_conds:\n",
    "            cond_df = item_df[item_df['condition']==this_cond]\n",
    "            us = np.unique(cond_df['sketch_id']) ## unique sketches in cell\n",
    "            if len(us)<4:\n",
    "                print(\"not enough in cell\", this_item, this_cond,len(us))\n",
    "                break\n",
    "            rand_sl = np.random.choice(us,size = 4,replace=False) ## list of random sketch ids\n",
    "            sample_sketches.append(rand_sl)\n",
    "            \n",
    "\n",
    "sample_sketches = [y for x in sample_sketches for y in x] ##flatten list\n",
    "            \n",
    "assert(len(np.unique(sample_sketches))==len(sample_sketches))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df = stroke_df[stroke_df['sketch_id'].isin(sample_sketches) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df.sketch_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Clear directories\n",
    "\n",
    "svg_dir = os.path.join(plot_dir,'triplet_sketches')\n",
    "png_dir =  os.path.join(plot_dir,'triplet_sketches_png')\n",
    "for this_dir in [svg_dir,png_dir]:\n",
    "    filelist = [ f for f in os.listdir(this_dir) ]\n",
    "    for this_sketch in filelist:\n",
    "        file_path = os.path.join(this_dir, this_sketch)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Render out SVGs and PNGs\n",
    "\n",
    "reload(srh)\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "\n",
    "    for sketch in render_df.sketch_id.unique():\n",
    "        this_sketch = render_df.query('sketch_id == @sketch')\n",
    "        svgs = list(this_sketch.svg)\n",
    "        srh.render_svg(svgs,out_dir =\"triplet_sketches\", base_dir=plot_dir,out_fname='{}.svg'.format(sketch))\n",
    "### Create path to svgs and convert to png for feature extraction\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "    svg_paths= srh.generate_svg_path_list(os.path.join(plot_dir,'triplet_sketches'))\n",
    "    srh.svg_to_png(svg_paths,out_dir=\"triplet_sketches_png\",base_dir=plot_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df_meta = pd.DataFrame(render_df.groupby(['sketch_id','category','target','label']).agg(num_strokes=pd.NamedAgg(column='stroke_id', aggfunc=lambda x: len(x.unique()))))\n",
    "render_df_meta=render_df_meta.reset_index()\n",
    "render_df_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df_meta.to_csv(index=False,path_or_buf=os.path.join(data_dir,'render_meta_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for CNN stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_birds = stroke_df[stroke_df['category']=='bird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_bj = D_birds[D_birds['target']=='bluejay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_tt = D_birds[D_birds['target']=='tomtit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(srh)\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "\n",
    "    for sketch in D_bj.sketch_id.unique():\n",
    "        this_sketch = D_bj.query('sketch_id == @sketch')\n",
    "        svgs = list(this_sketch.svg)\n",
    "        srh.render_svg(svgs,out_dir =\"train_bj\", base_dir=plot_dir,out_fname='{}.svg'.format(sketch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(srh)\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "\n",
    "    for sketch in D_tt.sketch_id.unique():\n",
    "        this_sketch = D_tt.query('sketch_id == @sketch')\n",
    "        svgs = list(this_sketch.svg)\n",
    "        srh.render_svg(svgs,out_dir=\"test_tt\" ,base_dir=plot_dir,out_fname='{}.svg'.format(sketch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create path to lesioned svgs and convert to png for feature extraction\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "    svg_paths= srh.generate_svg_path_list(os.path.join(plot_dir,'train_bj'))\n",
    "    srh.svg_to_png(svg_paths,out_dir=\"train_bj_png\",base_dir=plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create path to lesioned svgs and convert to png for feature extraction\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "    svg_paths= srh.generate_svg_path_list(os.path.join(plot_dir,'test_tt'))\n",
    "    srh.svg_to_png(svg_paths,out_dir=\"test_tt_png\",base_dir=plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set = os.path.join(plot_dir, \"train_bj_png\")\n",
    "\n",
    "\n",
    "train_set = datasets.ImageFolder(os.path.join(plot_dir), transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = datasets.ImageFolder(os.path.join(plot_dir), transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(6,16,kernel_size=5),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.ConvTranspose2d(16,6,kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(6,3,kernel_size=5),\n",
    "            nn.ReLU(True))\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #you can go for more epochs, I am using a mac\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().cpu()\n",
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = distance(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "  #  print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims= []\n",
    "for data in test_loader:\n",
    "    for d in data:\n",
    "        img, _ = data\n",
    "        p_img = model(img).detach()\n",
    "        for p in p_img:\n",
    "            ims.append(np.array(p).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims[25].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(ims[120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(ims[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base))",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
