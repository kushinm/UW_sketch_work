{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Author: Kushin Mukherjee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend running this notebook inside a conda environment to keep things organized and for reproducibility.\n",
    "\n",
    "Download and install conda: https://docs.conda.io/projects/conda/en/latest/user-guide/install/  \n",
    "Creating an environment: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\n",
    "\n",
    "Some tips for installing packages:\n",
    "First, activate your environment then install pip within the environment so that all the packages you install don't get installed to your global path. To do so:  \n",
    "Type `conda install pip` in your terminal\n",
    "\n",
    "Then, when in the project directory:  \n",
    "Type `pip install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using python 3 plus the latest versions of all the packages listed below. Be sure to update before running this nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from importlib import reload\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import svgpathtools\n",
    "import os.path\n",
    "from collections import Counter\n",
    "import svg_rendering_helpers as srh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "code_dir = os.getcwd()\n",
    "plot_dir = os.path.join(proj_dir,'plots')\n",
    "data_dir = os.path.join(proj_dir,'data')\n",
    "\n",
    "\n",
    "if not os.path.exists(code_dir):\n",
    "    os.makedirs(code_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "    \n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# if svg_rendering_helpers.py not in sys.path:\n",
    "#     sys.path.append(os.path.join(proj_dir,svg_rendering_helpers.py)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv(os.path.join(data_dir,'semantic_parts_annotated_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(string):\n",
    "    split_list = string.split(\"'\")\n",
    "    l=[\", u\",\"[u\",\"]\"]\n",
    "    out = [x for x in split_list if x not in l]\n",
    "    return(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.sketch_svg_string = D.sketch_svg_string.apply(listify)\n",
    "len(D.sketch_svg_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cats = np.unique(D.category)\n",
    "unique_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spline-level df where the modal label is set as the 'true' label for any given spline\n",
    "spline_df= D.groupby('spline_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "spline_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "##Creating a stroke-level dataframe that takes the mode value of annotation for its children splines to set as its\n",
    "##label value\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "stroke_svgs=OrderedDict()\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in np.unique(DS['sketch_id']):\n",
    "        DSS=DS[DS['sketch_id']==sketch]\n",
    "        for stroke in np.unique(DSS['stroke_num']):\n",
    "            DSA=DSS[DSS['stroke_num']==stroke]\n",
    "            DSA=DSA.reset_index()\n",
    "            stroke_svgs[DSA['stroke_id'][0]] = DSA['sketch_svg_string'][0][stroke]\n",
    "\n",
    "            \n",
    "            \n",
    "stroke_svg_df= pd.DataFrame.from_dict(stroke_svgs, orient='index')    \n",
    "stroke_group_data= D.groupby('stroke_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "labels= pd.DataFrame(stroke_group_data[['sketch_id','label','stroke_num','condition','target','category','outcome']])\n",
    "stroke_df=pd.merge(stroke_svg_df,labels,left_index=True, right_index =True)\n",
    "stroke_df.reset_index(level=0, inplace=True)\n",
    "stroke_df=stroke_df.rename(index=str, columns={\"index\": \"stroke_id\", 0: \"svg\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data for triplets task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to exclude some \"bad sketches\", which are mostly just handwritten text instead of drawingss\n",
    "\n",
    "bad_sketches = [\n",
    "'3058-fb4fe740-d862-453b-a08f-44375a040165_21',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_8',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_12',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_23',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_24',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_15',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_24',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_16',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_20',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_22',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_7',\n",
    "'3113-105e6653-7fd1-4451-af00-46bb3145880a_13',\n",
    "'6311-cd21a68a-f1df-4290-b744-b0c7c7c60ed8_5',\n",
    "'6786-9c3169eb-962e-468b-8922-b99247975eb2_32'\n",
    "]\n",
    "\n",
    "stroke_df = stroke_df[~stroke_df['sketch_id'].isin(bad_sketches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Currently constrained by minimum number of sketches in a conditionXcategoryXexemplar cell, which is 4\n",
    "### We have 2*4*8*4 (256) sketches in total\n",
    "\n",
    "random.seed(1022)\n",
    "sample_sketches= []\n",
    "\n",
    "for this_cat in unique_cats:\n",
    "    cat_df = stroke_df[stroke_df['category']== this_cat]\n",
    "    unique_items = np.unique(cat_df['target'])\n",
    "    for this_item in unique_items:\n",
    "        item_df = cat_df[cat_df['target']==this_item]\n",
    "        unique_conds = np.unique(item_df['condition'])\n",
    "        for this_cond in unique_conds:\n",
    "            cond_df = item_df[item_df['condition']==this_cond]\n",
    "            us = np.unique(cond_df['sketch_id']) ## unique sketches in cell\n",
    "            if len(us)<4:\n",
    "                print(\"not enough in cell\", this_item, this_cond,len(us))\n",
    "                break\n",
    "            rand_sl = np.random.choice(us,size = 4,replace=False) ## list of random sketch ids\n",
    "            sample_sketches.append(rand_sl)\n",
    "            \n",
    "\n",
    "sample_sketches = [y for x in sample_sketches for y in x] ##flatten list\n",
    "            \n",
    "assert(len(np.unique(sample_sketches))==len(sample_sketches))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df = stroke_df[stroke_df['sketch_id'].isin(sample_sketches) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df.sketch_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Clear directories\n",
    "\n",
    "svg_dir = os.path.join(plot_dir,'triplet_sketches')\n",
    "png_dir =  os.path.join(plot_dir,'triplet_sketches_png')\n",
    "for this_dir in [svg_dir,png_dir]:\n",
    "    filelist = [ f for f in os.listdir(this_dir) ]\n",
    "    for this_sketch in filelist:\n",
    "        file_path = os.path.join(this_dir, this_sketch)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Render out SVGs and PNGs\n",
    "\n",
    "reload(srh)\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "\n",
    "    for sketch in render_df.sketch_id.unique():\n",
    "        this_sketch = render_df.query('sketch_id == @sketch')\n",
    "        svgs = list(this_sketch.svg)\n",
    "        srh.render_svg(svgs,out_dir =\"triplet_sketches\", base_dir=plot_dir,out_fname='{}.svg'.format(sketch))\n",
    "### Create path to svgs and convert to png for feature extraction\n",
    "really_run = True\n",
    "\n",
    "if really_run==True:\n",
    "    svg_paths= srh.generate_svg_path_list(os.path.join(plot_dir,'triplet_sketches'))\n",
    "    srh.svg_to_png(svg_paths,out_dir=\"triplet_sketches_png\",base_dir=plot_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df_meta = pd.DataFrame(render_df.groupby(['sketch_id','category','target','label']).agg(num_strokes=pd.NamedAgg(column='stroke_id', aggfunc=lambda x: len(x.unique()))))\n",
    "render_df_meta=render_df_meta.reset_index()\n",
    "render_df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_df_meta.to_csv(index=False,path_or_buf=os.path.join(data_dir,'render_meta_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two feature analyses using VGG features extraced from UW (Tim + Pablo) feature extractor and Judy's feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tim and Pablo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base))",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
